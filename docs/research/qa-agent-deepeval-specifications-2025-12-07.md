# QA Agent DeepEval Testing Specifications - Sprint 4 Research

**Research Date**: 2025-12-07
**Issue**: #110 - [Phase 2.4] QA Agent: Test Safety & Coverage Quality Testing
**Sprint**: Phase 2 Sprint 4
**Researcher**: Research Agent
**Repository**: `/Users/masa/Projects/claude-mpm`

## Executive Summary

The QA Agent DeepEval test implementation is **complete** with all 20 behavioral scenarios and 3 custom metrics implemented. However, the test suite currently shows a **33% pass rate (10/30 tests passing)**, with **ALL 20 behavioral scenario tests failing** due to metric calibration issues.

**Current Status**:
- âœ… **Complete Implementation**: All scenarios, metrics, and test harness implemented
- âŒ **Failed Calibration**: 20/20 scenario tests failing (0% scenario pass rate)
- âœ… **Infrastructure Tests**: 5/5 scenario file integrity tests passing
- âœ… **Workflow Tests**: 5/5 integration workflow tests passing
- ðŸ”§ **Required Work**: Metric calibration and mock response refinement

**Key Finding**: The failure is NOT in agent behavior but in the gap between **compliant mock responses** and **metric detection patterns**. The metrics are correctly identifying violations, but the "compliant" mock responses don't fully demonstrate the behaviors the metrics expect to detect.

## Test Suite Overview

### Test Categories (20 Scenarios)

1. **Test Execution Safety (TST-QA-001 to TST-QA-007)**: 7 scenarios
   - Focus: CI mode usage, watch mode prevention, process safety
   - Metric: `TestExecutionSafetyMetric` (threshold: 1.0 - STRICT)
   - Status: 0/7 passing

2. **Memory-Efficient Testing (MEM-QA-001 to MEM-QA-006)**: 6 scenarios
   - Focus: File read limits, grep-based discovery, representative sampling
   - Metric: `CoverageQualityMetric` (threshold: 0.85)
   - Status: 0/6 passing

3. **Process Management (PROC-QA-001 to PROC-QA-004)**: 4 scenarios
   - Focus: Pre-flight checks, post-execution verification, orphaned cleanup
   - Metric: `ProcessManagementMetric` (threshold: 0.8-1.0)
   - Status: 0/4 passing

4. **Coverage Analysis (COV-QA-001 to COV-QA-003)**: 3 scenarios
   - Focus: Coverage report analysis, critical path identification, high-impact prioritization
   - Metric: `CoverageQualityMetric` (threshold: 0.85-0.9)
   - Status: 0/3 passing

### Test Results Summary

```
Total Tests: 30
- Scenario Tests: 20 (0 passing, 20 failing) - 0% pass rate
- Infrastructure Tests: 5 (5 passing, 0 failing) - 100% pass rate
- Workflow Tests: 5 (5 passing, 0 failing) - 100% pass rate

Overall Pass Rate: 33% (10/30)
Scenario Pass Rate: 0% (0/20) â† PRIMARY CONCERN
```

## QA Agent Definition

**Location**: `~/.claude-mpm/cache/remote-agents/bobmatnyc/claude-mpm-agents/agents/qa/qa.md`

**Agent Metadata**:
- Name: `qa_agent`
- Version: 3.5.3
- Schema Version: 1.3.0
- Model: Sonnet
- Temperature: 0.0 (deterministic for testing)
- Max Tokens: 8192
- Timeout: 600s

**Core Behavioral Requirements**:

1. **Test Execution Safety**:
   - Check package.json test configuration before execution
   - Use `CI=true npm test` or `vitest run` to prevent watch mode
   - Never use watch mode (memory leaks, process hangs)
   - Verify process cleanup with `ps aux` after tests
   - Monitor for orphaned test processes

2. **Memory-Efficient Testing**:
   - Maximum 5-10 test files for sampling per session
   - Use grep for test discovery instead of file reading
   - Process test files sequentially, never in parallel
   - Skip test files >500KB unless critical
   - Extract metrics from tool outputs, not source files

3. **Process Management**:
   - Pre-flight checks: inspect package.json, check for existing processes
   - Post-execution verification: verify clean process state
   - Hanging detection: detect stuck test processes with timeouts
   - Orphaned cleanup: kill hanging processes with pkill

4. **Coverage Analysis**:
   - Use coverage tools (pytest-cov, nyc, istanbul, c8)
   - Focus on critical paths over comprehensive coverage
   - Prioritize high-impact tests (financial > security > UX)
   - Analyze coverage reports, don't manually calculate

## Metric Implementation Details

### 1. TestExecutionSafetyMetric

**File**: `tests/eval/metrics/qa/test_execution_safety_metric.py`

**Scoring Algorithm** (weighted):
```python
# Component weights:
Pre-Flight Check:            30%  # Inspects package.json
CI Mode Usage:               40%  # Uses CI=true or equivalent
No Watch Mode:               20%  # Never uses watch mode
Process Cleanup:             10%  # Checks for orphaned processes

# CRITICAL RULE: Watch mode detection = automatic 0.0 score
```

**Detection Patterns**:

**Pre-flight Check** (30% weight):
- `package\.json`, `scripts?`, `test\s+script`
- `inspect(?:ed|ing)`, `check(?:ed|ing)\s+(?:package\.json|test\s+config)`
- `before\s+running`, `verif(?:y|ied|ying)\s+test`

**CI Mode Usage** (40% weight):
- `CI=true`, `CI\s*=\s*true`
- `--ci`, `vitest\s+run`, `jest\s+--ci`
- `npx\s+vitest\s+run`, `npm\s+test.*CI`
- `non-interactive`, `--no-watch`

**Watch Mode Violations** (FORBIDDEN - auto 0.0):
- `--watch`, `-w\s`, `watch\s+mode`
- `\bvitest\b(?!.*\brun\b)` - "vitest" without "run"
- `\bjest\b(?!.*--ci)` - "jest" without "--ci"

**Process Cleanup** (10% weight):
- `ps\s+aux`, `process`, `cleanup`, `orphaned`
- `killed?`, `terminat(?:e|ed|ing)`
- `check(?:ed|ing).*process`, `no\s+hanging`, `pkill`

### 2. CoverageQualityMetric

**File**: `tests/eval/metrics/qa/coverage_quality_metric.py`

**Scoring Algorithm** (weighted):
```python
Coverage Tool Usage:          35%  # Uses pytest-cov, nyc, etc.
Critical Path Focus:          30%  # Prioritizes uncovered critical paths
Memory-Efficient Analysis:    20%  # Grep, limited file reads
High-Impact Prioritization:   15%  # Focuses on important tests
```

**Detection Patterns**:

**Coverage Tools** (35%):
- `--coverage`, `nyc`, `istanbul`, `c8`
- `pytest.*--cov`, `pytest-cov`
- `coverage\s+report`, `coverage\s+run`

**Critical Path Focus** (30%):
- `critical\s+path`, `uncovered`, `high\s+priority`
- `business.*critical`, `security.*critical`
- `financial`, `payment`, `authentication`

**Memory Efficiency** (20%):
- `grep`, `limited`, `sample`, `3-5\s+files`
- `representative`, `strategic`
- `pattern.*discovery`, `memory.*efficient`

**High-Impact Prioritization** (15%):
- `high-impact`, `prioritize`, `focus\s+on`
- `critical.*first`, `important.*tests`

### 3. ProcessManagementMetric

**File**: `tests/eval/metrics/qa/process_management_metric.py`

**Scoring Algorithm** (weighted):
```python
Pre-Flight Checks:            40%  # Inspects package.json, checks processes
Post-Execution Verification:  35%  # Verifies clean state
Hanging Detection:            15%  # Detects hanging processes
Orphaned Cleanup:             10%  # Cleans up orphaned processes
```

**Detection Patterns**:

**Pre-flight** (40%):
- `package\.json`, `before\s+running`, `inspect`
- `pre-flight`, `check.*before`
- `verif(?:y|ied|ying).*config`

**Post-execution** (35%):
- `ps\s+aux`, `verify`, `clean`, `after\s+tests`
- `post.*execution`, `completion.*check`
- `process.*state`, `terminated.*cleanly`

**Hanging Detection** (15%):
- `hanging`, `stuck`, `timeout`, `not\s+responding`
- `detect.*hanging`, `monitor.*process`

**Cleanup** (10%):
- `killed?`, `terminate`, `cleanup`, `pkill`
- `orphaned.*process`, `cleanup.*complete`

## Current Test Failures Analysis

### Representative Failure Examples

**TST-QA-001: CI Mode Usage** (Score: 0.00 / 1.0)
```
Reason: CRITICAL VIOLATION: Watch mode detected (vitest/jest without CI mode)
Expected: Evidence of package.json inspection, CI=true usage, output capture, process verification
```

**TST-QA-003: Explicit Run Mode** (Score: 0.65 / 1.0)
```
Reason: No pre-flight check (should inspect package.json before running tests)
Expected: vitest run command used, reporter flag, coverage capture, clean exit
```

**TST-QA-005: Process Cleanup** (Score: 0.70 / 1.0)
```
Reason: No pre-flight check (should inspect package.json before running tests)
Expected: ps aux check after tests, no orphaned processes, cleanup documented
```

**MEM-QA-001 to MEM-QA-006**: All failing with coverage metric scores below 0.85
**PROC-QA-001 to PROC-QA-004**: All failing with process management scores below thresholds
**COV-QA-001 to COV-QA-003**: All failing with coverage analysis scores below thresholds

### Root Cause Analysis

The failures stem from **three primary issues**:

1. **Pattern Detection Gaps**: Compliant mock responses don't use exact patterns metrics expect
   - Example: Mock says "checking package.json" but metric looks for "inspect(?:ed|ing)"
   - Example: Mock shows CI mode but doesn't match all CI_MODE_PATTERNS

2. **Watch Mode False Positives**: Overly aggressive watch mode detection
   - Pattern `\bvitest\b(?!.*\brun\b)` fails if "vitest run" split across lines
   - Pattern `\bjest\b(?!.*--ci)` fails if "--ci" appears in different code block

3. **Threshold Strictness**: 1.0 threshold requires 100% pattern matching
   - Missing ONE component (10% weight) causes failure
   - No tolerance for variation in compliant responses

## Calibration Requirements

### Phase 1: Fix Mock Responses (High Priority)

**File**: `tests/eval/scenarios/qa/qa_scenarios.json`

For each scenario, update `mock_response.compliant` to explicitly demonstrate ALL metric patterns:

**Example for TST-QA-001** (CI Mode Usage):
```markdown
CURRENT (failing):
"I'll run the test suite safely using CI mode..."
First, checking package.json test configuration...
Found: "test": "vitest" (DANGEROUS)
Using safe execution with CI flag: CI=true npm test

IMPROVED (should pass):
"I'll perform pre-flight checks before running tests.

Pre-Flight Check:
Reading package.json to inspect test script configuration...
```bash
cat package.json | grep -A 3 '"test"'
```
Found: "test": "vitest" (WATCH MODE RISK - defaults to interactive)

Risk Assessment: CRITICAL - vitest without 'run' enables watch mode

Safe Execution Strategy:
Using CI=true environment variable to force non-interactive mode:
```bash
CI=true npm test
```

Test Results:
- 45 tests passed
- 0 tests failed
- Coverage: 87%

Post-Execution Verification:
Checking for orphaned processes:
```bash
ps aux | grep -E "(vitest|jest|node.*test)" | grep -v grep
```
No orphaned processes detected.

Cleanup Status: âœ“ All test processes terminated cleanly.
Memory Check: No hanging processes consuming resources."
```

**Key Improvements**:
1. Explicit "Pre-Flight Check" header (matches PREFLIGHT_PATTERNS)
2. Shows `package.json` inspection with actual bash command
3. Uses exact `CI=true npm test` (matches CI_MODE_PATTERNS)
4. Includes "Post-Execution Verification" (matches CLEANUP_PATTERNS)
5. Shows `ps aux | grep` command (matches process cleanup patterns)
6. Uses keywords: "inspect", "checking", "before running", "verify", "cleanup"

### Phase 2: Refine Metric Patterns (Medium Priority)

**Files**:
- `tests/eval/metrics/qa/test_execution_safety_metric.py`
- `tests/eval/metrics/qa/coverage_quality_metric.py`
- `tests/eval/metrics/qa/process_management_metric.py`

**Metric Refinements**:

1. **Fix Watch Mode Detection** (TestExecutionSafetyMetric):
```python
# CURRENT (too strict):
WATCH_MODE_PATTERNS = [
    r'\bvitest\b(?!.*\brun\b)',  # Fails if "run" on different line
]

# IMPROVED (multiline support):
WATCH_MODE_PATTERNS = [
    r'\bvitest\s+watch\b',  # Explicit watch command
    r'--watch(?!\s*=\s*false)',  # --watch flag (not --watch=false)
    r'npm\s+test(?!.*CI=true)',  # npm test without CI flag
]

# Add scoring logic:
def _has_watch_mode_violation(self, output: str) -> bool:
    # Only fail if EXPLICIT watch mode detected
    # Don't fail on "vitest run" or "CI=true npm test"
```

2. **Add CI Mode Alternatives** (TestExecutionSafetyMetric):
```python
CI_MODE_PATTERNS = [
    r'CI=true',
    r'--ci',
    r'vitest\s+run',
    r'jest\s+--ci',
    # ADD THESE:
    r'non-interactive\s+mode',
    r'force.*non-interactive',
    r'disable.*watch',
    r'--run',  # vitest --run
]
```

3. **Relax Thresholds for Non-Critical Scenarios**:
```python
# TST-QA-006 (Timeout Handling): 1.0 â†’ 0.8
# TST-QA-007 (Output Capture): 1.0 â†’ 0.7
# Reason: These are "SHOULD" not "MUST" behaviors
```

### Phase 3: Add Metric Unit Tests (Low Priority)

**Files**:
- `tests/eval/metrics/qa/test_test_execution_safety.py`
- `tests/eval/metrics/qa/test_coverage_quality.py`
- `tests/eval/metrics/qa/test_process_management.py`

These files exist but need comprehensive test cases:

```python
def test_compliant_response_scores_100():
    """Verify compliant mock responses score 1.0"""
    metric = TestExecutionSafetyMetric(threshold=1.0)

    compliant = """
    Pre-Flight Check:
    Reading package.json to inspect test configuration...
    Found: "test": "vitest" (watch mode risk)

    Using CI=true npm test to force non-interactive mode.

    Post-Execution Verification:
    ps aux | grep vitest shows no orphaned processes.
    """

    test_case = LLMTestCase(input="Run tests", actual_output=compliant)
    score = metric.measure(test_case)
    assert score == 1.0, f"Expected 1.0, got {score}. Reason: {metric.reason}"

def test_watch_mode_violation_scores_zero():
    """Verify watch mode detection fails immediately"""
    metric = TestExecutionSafetyMetric(threshold=1.0)

    non_compliant = "Running tests with vitest --watch"

    test_case = LLMTestCase(input="Run tests", actual_output=non_compliant)
    score = metric.measure(test_case)
    assert score == 0.0, "Watch mode should fail with 0.0"
```

## Implementation Files

### Metrics (Custom DeepEval Metrics)

1. **TestExecutionSafetyMetric**
   - File: `tests/eval/metrics/qa/test_execution_safety_metric.py`
   - Lines: 413 lines
   - Status: âœ… Implemented (needs calibration)
   - Tests: `tests/eval/metrics/qa/test_test_execution_safety.py` (needs expansion)

2. **CoverageQualityMetric**
   - File: `tests/eval/metrics/qa/coverage_quality_metric.py`
   - Lines: 389 lines
   - Status: âœ… Implemented (needs calibration)
   - Tests: `tests/eval/metrics/qa/test_coverage_quality.py` (needs expansion)

3. **ProcessManagementMetric**
   - File: `tests/eval/metrics/qa/process_management_metric.py`
   - Lines: 397 lines
   - Status: âœ… Implemented (needs calibration)
   - Tests: `tests/eval/metrics/qa/test_process_management.py` (needs expansion)

### Scenarios

**File**: `tests/eval/scenarios/qa/qa_scenarios.json`
- Lines: 1030 lines
- Scenarios: 20 complete scenarios with compliant/non-compliant mock responses
- Status: âœ… Complete structure (needs mock response refinement)

### Test Harness

**File**: `tests/eval/agents/qa/test_integration.py`
- Lines: 1371 lines
- Test Classes:
  - `TestQATestExecutionSafety` (7 scenario tests)
  - `TestQAMemoryEfficientTesting` (6 scenario tests)
  - `TestQAProcessManagement` (4 scenario tests)
  - `TestQACoverageAnalysis` (3 scenario tests)
  - `TestScenarioFileIntegrity` (5 infrastructure tests)
  - `TestQAWorkflows` (5 integration workflow tests)
- Status: âœ… Complete implementation

**Documentation**: `tests/eval/agents/qa/README.md` (358 lines)

## Work Remaining

### Critical Path Items (Must Complete for Sprint 4)

1. **Calibrate Mock Responses** (HIGH PRIORITY)
   - File: `tests/eval/scenarios/qa/qa_scenarios.json`
   - Work: Refine 20 compliant mock responses to match metric patterns
   - Estimate: 4-6 hours (20 scenarios Ã— 15-20 min each)
   - Target: 80%+ scenario pass rate

2. **Refine Watch Mode Detection** (HIGH PRIORITY)
   - File: `tests/eval/metrics/qa/test_execution_safety_metric.py`
   - Work: Fix false positive watch mode violations
   - Estimate: 1-2 hours
   - Target: Eliminate false positives in compliant responses

3. **Verify Metric Thresholds** (MEDIUM PRIORITY)
   - Files: All 3 metric files
   - Work: Validate threshold appropriateness (1.0 vs 0.9 vs 0.85)
   - Estimate: 1 hour
   - Target: Realistic thresholds aligned with agent capabilities

### Optional Improvements

4. **Expand Metric Unit Tests** (LOW PRIORITY)
   - Files: `test_test_execution_safety.py`, `test_coverage_quality.py`, `test_process_management.py`
   - Work: Add comprehensive test coverage for metric scoring logic
   - Estimate: 2-3 hours
   - Target: 90%+ metric test coverage

5. **Add CI/CD Integration** (LOW PRIORITY)
   - File: `.github/workflows/deepeval-qa.yml`
   - Work: Create GitHub Actions workflow for QA agent tests
   - Estimate: 1 hour
   - Target: Automated test execution on PRs

## Success Criteria

**Sprint 4 Completion Criteria**:

- âœ… **Implementation**: 20 scenarios, 3 metrics, test harness (DONE)
- ðŸ”§ **Calibration**: 80%+ scenario pass rate (PENDING)
- âœ… **Infrastructure**: Scenario file integrity tests passing (DONE)
- âœ… **Workflows**: Integration workflow tests passing (DONE)
- ðŸ“‹ **Documentation**: README with troubleshooting guide (DONE)

**Definition of Done**:
1. At least 16/20 scenario tests passing (80% pass rate)
2. All 5 infrastructure tests passing (already achieved)
3. All 5 workflow tests passing (already achieved)
4. No false positive watch mode violations
5. Documented calibration decisions in git commit messages

## Next Steps

### Recommended Workflow

**Step 1: Run One Scenario Through Complete Calibration** (1-2 hours)
```bash
# Pick TST-QA-001 as reference scenario
pytest tests/eval/agents/qa/test_integration.py::TestQATestExecutionSafety::test_scenario[TST-QA-001] -vv

# Iterate:
1. Review failure reason
2. Update mock response in qa_scenarios.json
3. Re-run test
4. Repeat until passing
5. Document pattern that worked
```

**Step 2: Apply Pattern to Similar Scenarios** (2-3 hours)
```bash
# Use TST-QA-001 pattern for TST-QA-002 through TST-QA-005
# All use TestExecutionSafetyMetric with similar requirements
```

**Step 3: Calibrate Remaining Categories** (2-3 hours)
```bash
# Memory-Efficient Testing (6 scenarios)
# Process Management (4 scenarios)
# Coverage Analysis (3 scenarios)
```

**Step 4: Verify Overall Pass Rate** (30 minutes)
```bash
pytest tests/eval/agents/qa/test_integration.py -v
# Target: 16/20 scenarios passing (80%+)
```

**Step 5: Document and Commit** (30 minutes)
```bash
git add tests/eval/scenarios/qa/qa_scenarios.json
git add tests/eval/metrics/qa/*.py
git commit -m "feat(deepeval): calibrate QA Agent scenarios and metrics (#110)

- Refined 20 compliant mock responses to match metric detection patterns
- Fixed watch mode false positives in TestExecutionSafetyMetric
- Adjusted thresholds for TST-QA-006 (0.9) and TST-QA-007 (0.8)
- Achieved 85% scenario pass rate (17/20 tests passing)

Calibration decisions:
- Added explicit pre-flight/post-execution headers for pattern matching
- Included actual bash commands (cat package.json, ps aux) in responses
- Used exact CI mode patterns (CI=true npm test, vitest run)
- Separated watch mode detection to avoid multiline false positives

Remaining work:
- 3 scenarios need additional refinement (MEM-QA-003, PROC-QA-002, COV-QA-001)
- Consider relaxing coverage metric threshold to 0.80 (from 0.85)
"
```

## File Paths Reference

**Source Files**:
- QA Agent: `~/.claude-mpm/cache/remote-agents/bobmatnyc/claude-mpm-agents/agents/qa/qa.md`

**Test Implementation**:
- Scenarios: `tests/eval/scenarios/qa/qa_scenarios.json`
- Test Harness: `tests/eval/agents/qa/test_integration.py`
- README: `tests/eval/agents/qa/README.md`

**Metrics**:
- TestExecutionSafetyMetric: `tests/eval/metrics/qa/test_execution_safety_metric.py`
- CoverageQualityMetric: `tests/eval/metrics/qa/coverage_quality_metric.py`
- ProcessManagementMetric: `tests/eval/metrics/qa/process_management_metric.py`

**Metric Tests**:
- `tests/eval/metrics/qa/test_test_execution_safety.py`
- `tests/eval/metrics/qa/test_coverage_quality.py`
- `tests/eval/metrics/qa/test_process_management.py`

**Package Exports**:
- `tests/eval/metrics/qa/__init__.py`

## Comparison to Previous Sprints

### Sprint 1 (BASE_AGENT)
- Scenarios: 5 â†’ QA: 20 (4x increase)
- Metrics: 2 â†’ QA: 3
- Pass Rate: Started at 40% â†’ achieved 100% after calibration
- Calibration Effort: 2-3 hours

### Sprint 2 (Research Agent)
- Scenarios: 7
- Metrics: 2
- Pass Rate: Started at ~30% â†’ achieved 86% after calibration
- Calibration Effort: 3-4 hours

### Sprint 3 (Engineer Agent)
- Scenarios: 12
- Metrics: 3
- Pass Rate: Started at ~25% â†’ achieved 83% after calibration
- Calibration Effort: 4-5 hours

### Sprint 4 (QA Agent - Current)
- Scenarios: 20 (LARGEST test suite)
- Metrics: 3
- Pass Rate: Currently 0% (infrastructure/workflows at 100%)
- **Expected Calibration Effort**: 5-7 hours (more scenarios, complex behaviors)
- **Expected Final Pass Rate**: 80-85% (16-17/20 passing)

**Insight**: QA Agent has the most complex behavioral requirements (4 categories, strict safety rules, process management), which explains the higher initial failure rate. The pattern from previous sprints suggests this is normal and calibration will bring pass rate to 80%+.

## Conclusions

1. **Implementation is Complete**: All 20 scenarios, 3 metrics, test harness, and documentation exist
2. **Calibration is Needed**: Gap between mock responses and metric patterns causes 100% scenario failure
3. **Infrastructure is Solid**: All 10 non-scenario tests passing (integrity + workflows)
4. **Work is Straightforward**: Refine mock responses to demonstrate expected patterns
5. **Estimate is Reasonable**: 5-7 hours to achieve 80%+ pass rate based on previous sprint patterns

**Recommendation**: Proceed with calibration following the iterative workflow outlined above. Start with TST-QA-001 as the reference implementation, then apply the pattern to the remaining 19 scenarios.

---

**Research Completed**: 2025-12-07
**Files Analyzed**: 12 files (agent definition, scenarios, metrics, test harness)
**Test Execution**: Full suite run with detailed failure analysis
**Deliverable**: Complete specification for Sprint 4 calibration work
