name: DeepEval Agent Tests

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  deepeval-base-agent:
    name: BASE_AGENT DeepEval Tests
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[eval,dev]"

      - name: Run BASE_AGENT metric tests
        run: |
          pytest tests/eval/metrics/base_agent/ -v --tb=short

      - name: Run BASE_AGENT harness tests
        run: |
          pytest tests/eval/agents/base_agent/ -v --tb=short

      - name: Generate BASE_AGENT test summary
        if: always()
        run: |
          echo "## BASE_AGENT Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Metric Tests: tests/eval/metrics/base_agent/" >> $GITHUB_STEP_SUMMARY
          echo "- Harness Tests: tests/eval/agents/base_agent/" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total Tests:** 89 (39 metric + 45 harness + 5 integration)" >> $GITHUB_STEP_SUMMARY

      - name: Upload BASE_AGENT test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-base-agent-results-${{ matrix.python-version }}
          path: |
            .pytest_cache/
            htmlcov/
          retention-days: 7

  deepeval-engineer-agent:
    name: Engineer Agent DeepEval Tests
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[eval,dev]"

      - name: Run Engineer Agent metric tests
        run: |
          pytest tests/eval/metrics/engineer/ -v --tb=short

      - name: Run Engineer Agent scenario tests
        run: |
          pytest tests/eval/agents/engineer/test_integration.py -v --tb=short -k "not TestEngineerWorkflows"

      - name: Run Engineer Agent workflow integration tests
        run: |
          pytest tests/eval/agents/engineer/test_integration.py::TestEngineerWorkflows -v --tb=short --timeout=300

      - name: Generate Engineer Agent test summary
        if: always()
        run: |
          echo "## Engineer Agent Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Metric Tests: tests/eval/metrics/engineer/" >> $GITHUB_STEP_SUMMARY
          echo "- Scenario Tests: tests/eval/agents/engineer/ (25 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: TestEngineerWorkflows (5 tests)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total Tests:** 39 (9 metric + 25 scenarios + 5 integration)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Categories Tested" >> $GITHUB_STEP_SUMMARY
          echo "- Code Minimization (10 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Consolidation & Duplicate Elimination (7 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Anti-Pattern Avoidance (5 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Test Process Management (3 scenarios)" >> $GITHUB_STEP_SUMMARY

      - name: Upload Engineer Agent test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-engineer-agent-results-${{ matrix.python-version }}
          path: |
            .pytest_cache/
            htmlcov/
          retention-days: 7

  deepeval-qa-agent:
    name: QA Agent DeepEval Tests
    runs-on: ubuntu-latest
    needs: [deepeval-base-agent]  # Depends on base tests passing

    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[eval,dev]"

      - name: Run QA Agent metric tests
        run: |
          pytest tests/eval/metrics/qa/ -v --tb=short

      - name: Run QA Agent scenario tests
        run: |
          pytest tests/eval/agents/qa/test_integration.py -v --tb=short -k "not TestQAWorkflows"

      - name: Run QA Agent workflow integration tests
        run: |
          pytest tests/eval/agents/qa/test_integration.py::TestQAWorkflows -v --tb=short --timeout=300

      - name: Generate QA Agent test summary
        if: always()
        run: |
          echo "## QA Agent Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Metric Tests: tests/eval/metrics/qa/" >> $GITHUB_STEP_SUMMARY
          echo "- Scenario Tests: tests/eval/agents/qa/ (20 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: TestQAWorkflows (5 tests)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total Tests:** 67 (42 metric + 20 scenarios + 5 integration)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Categories Tested" >> $GITHUB_STEP_SUMMARY
          echo "- Test Execution Safety (7 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Memory-Efficient Testing (6 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Process Management (4 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Coverage Analysis (3 scenarios)" >> $GITHUB_STEP_SUMMARY

      - name: Upload QA Agent test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-qa-agent-results-${{ matrix.python-version }}
          path: |
            .pytest_cache/
            htmlcov/
          retention-days: 7

  deepeval-ops-agent:
    name: Ops Agent DeepEval Tests
    runs-on: ubuntu-latest
    needs: [deepeval-qa-agent]

    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[eval,dev]"

      - name: Run Ops Agent metric tests
        run: |
          pytest tests/eval/metrics/ops/ -v --tb=short

      - name: Run Ops Agent scenario tests
        run: |
          pytest tests/eval/agents/ops/test_integration.py -v --tb=short -k "not TestOpsWorkflows"

      - name: Run Ops Agent workflow integration tests
        run: |
          pytest tests/eval/agents/ops/test_integration.py::TestOpsWorkflows -v --tb=short --timeout=300

      - name: Generate Ops Agent test summary
        if: always()
        run: |
          echo "## Ops Agent Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Metric Tests: tests/eval/metrics/ops/ (38 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- Scenario Tests: tests/eval/agents/ops/ (18 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: TestOpsWorkflows (5 tests)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total Tests:** 61 (38 metric + 18 scenarios + 5 integration)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Categories Tested" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment Protocol (6 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Infrastructure Focus (5 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Security Emphasis (4 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Verification Requirements (3 scenarios)" >> $GITHUB_STEP_SUMMARY

      - name: Upload Ops Agent test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-ops-agent-results-${{ matrix.python-version }}
          path: |
            .pytest_cache/
            htmlcov/
          retention-days: 7

  deepeval-documentation-agent:
    name: Documentation Agent DeepEval Tests
    runs-on: ubuntu-latest
    needs: [deepeval-ops-agent]

    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[eval,dev]"

      - name: Run Documentation Agent metric tests
        run: |
          pytest tests/eval/metrics/documentation/ -v --tb=short

      - name: Run Documentation Agent scenario tests
        run: |
          pytest tests/eval/agents/documentation/test_integration.py -v --tb=short -k "not TestDocumentationWorkflows"

      - name: Run Documentation Agent workflow integration tests
        run: |
          pytest tests/eval/agents/documentation/test_integration.py::TestDocumentationWorkflows -v --tb=short --timeout=300

      - name: Generate Documentation Agent test summary
        if: always()
        run: |
          echo "## Documentation Agent Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Metric Tests: tests/eval/metrics/documentation/ (41 tests)" >> $GITHUB_STEP_SUMMARY
          echo "- Scenario Tests: tests/eval/agents/documentation/ (12 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: TestDocumentationWorkflows (3 tests)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total Tests:** 56 (41 metric + 12 scenarios + 3 integration)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Categories Tested" >> $GITHUB_STEP_SUMMARY
          echo "- Clarity Standards (4 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Audience Awareness (4 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Maintenance Focus (2 scenarios)" >> $GITHUB_STEP_SUMMARY
          echo "- Completeness Requirements (2 scenarios)" >> $GITHUB_STEP_SUMMARY

      - name: Upload Documentation Agent test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deepeval-documentation-agent-results-${{ matrix.python-version }}
          path: |
            .pytest_cache/
            htmlcov/
          retention-days: 7
